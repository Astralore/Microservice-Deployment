# [æœ¬åœ°] llm_direct.py
import requests
import json

class DirectLLM:
    # å…¼å®¹æ€§æ„é€ å‡½æ•°ï¼šå³ä½¿ä¼ å…¥ model_path ä¹Ÿä¼šè¢«å¿½ç•¥ï¼Œé˜²æ­¢æŠ¥é”™
    def __init__(self, base_model_path=None, lora_path=None, api_url="http://localhost:6006/predict"):
        self.api_url = api_url
        # å¦‚æœé€šè¿‡ SSH éš§é“æ˜ å°„äº†ç«¯å£ï¼Œlocalhost:6006 å°±æ˜¯äº‘ç«¯çš„ 6006
        print(f"ğŸ”— Remote LLM Bridge Initialized -> {self.api_url}")

    def get_suggestions(self, description):
        if not description:
            return []
            
        try:
            payload = {'description': description}
            # è®¾ç½® 15ç§’è¶…æ—¶ï¼Œé˜²æ­¢ç½‘ç»œæ³¢åŠ¨å¯¼è‡´ä»¿çœŸå¡æ­»
            response = requests.post(self.api_url, json=payload, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                # å‡è®¾äº‘ç«¯è¿”å›æ ¼å¼ä¸º {"node_ids": [1, 2, 3]}
                suggestions = data.get('node_ids', [])
                if suggestions:
                    # å¯é€‰ï¼šæ‰“å°ä¸€ä¸‹æ—¥å¿—ç¡®è®¤æ”¶åˆ°æ¨è
                    # print(f" [LLM Recvd] {suggestions}")
                    pass
                return suggestions
            else:
                print(f"âš ï¸ Remote LLM Error (Status: {response.status_code})")
                return []
                
        except requests.exceptions.ConnectionError:
            print("âš ï¸ Connection Refused: è¯·æ£€æŸ¥ SSH éš§é“æ˜¯å¦å¼€å¯ (ssh -L 6006:...)")
            return []
        except Exception as e:
            print(f"âš ï¸ Network Error: {e}")
            return []