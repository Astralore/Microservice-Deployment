# evaluate_local.py
import time
import numpy as np
import paddle
import os
import json
from environment_client import EnvironmentClient
from agent import DuelingDQNAgent 
from llm_direct import DirectLLM 
from config import ACTION_DIM # è¿™é‡Œçš„ ACTION_DIM åº”è¯¥æ˜¯ 50 (è®­ç»ƒæ—¶çš„é…ç½®)

# [é…ç½®]
# è¯·ç¡®è®¤è·¯å¾„æ­£ç¡®
RL_MODEL_PATH = "D:/Code/MD_DATA/experiments/20251215_133758/dueling_dqn_model.pdparams"
ENABLE_LLM = True 

# è®­ç»ƒæ—¶çš„èŠ‚ç‚¹æ•°é‡ (RL çš„è§†ç•Œè¾¹ç•Œ)
TRAIN_NODE_COUNT = 49

class SafeEnvironmentClient(EnvironmentClient):
    """ å¤„ç†ç»´åº¦ä¸åŒ¹é…çš„å®¢æˆ·ç«¯ """
    def parse_state_adaptive(self, raw_state_vec, raw_mask):
        current_nodes = len(raw_mask)
        target_nodes = TRAIN_NODE_COUNT 
        
        global_feats = raw_state_vec[-2:] 
        node_feats = raw_state_vec[:-2]   
        
        if current_nodes >= target_nodes:
            # [åœºæ™¯: èŠ‚ç‚¹å¢åŠ ] æˆªæ–­ç»™ RL çœ‹
            sliced_feats = node_feats[:target_nodes * 3]
            s_out = np.concatenate([sliced_feats, global_feats])
            m_out = raw_mask[:target_nodes]
        else:
            # [åœºæ™¯: èŠ‚ç‚¹å‡å°‘] è¡¥é›¶å¡«å…… (è™½ç„¶æœ¬æ¬¡å®éªŒç”¨ä¸åˆ°ï¼Œä½†ä¸ºäº†å¥å£®æ€§ä¿ç•™)
            pad_nodes = target_nodes - current_nodes
            dummy_feat = np.array([1.0, 1.0, -1.0] * pad_nodes, dtype=np.float32)
            s_out = np.concatenate([node_feats, dummy_feat, global_feats])
            dummy_mask = np.zeros(pad_nodes, dtype=bool)
            m_out = np.concatenate([raw_mask, dummy_mask])
            
        return s_out.astype('float32'), m_out.astype('bool')

def evaluate():
    print(f"ğŸš€ Starting Evaluation (Nodes: {ACTION_DIM} -> Generalized, LLM={ENABLE_LLM})...")
    env = SafeEnvironmentClient()
    
    agent = DuelingDQNAgent()
    if os.path.exists(RL_MODEL_PATH):
        try:
            agent.main_network.set_state_dict(paddle.load(RL_MODEL_PATH))
            print(f"âœ… RL Model Loaded")
        except:
            print("âŒ Model Load Failed, using random weights.")
    else:
        print("âš ï¸ No model found.")

    llm = DirectLLM() # è¿æ¥ localhost:6006

    for episode in range(1, 6): # è·‘ 5 è½®çœ‹çœ‹æ•ˆæœ
        # Reset å¾—åˆ°çš„æ˜¯ 61 ä¸ªèŠ‚ç‚¹çš„åŸå§‹æ•°æ®
        raw_state, raw_mask, info = env.reset()
        current_desc = info.get('description', "")
        
        # é€‚é…ç»™ RL (åªç»™å®ƒçœ‹å‰ 50 ä¸ª)
        state_for_rl, mask_for_rl = env.parse_state_adaptive(raw_state, raw_mask)
        
        total_reward = 0
        step = 0
        stats = {"RL_Native": 0, "LLM_NewNode": 0, "LLM_OldNode_Opt": 0}
        
        while True:
            step += 1
            
            # --- 1. RL è®¡ç®— Q å€¼ (åŸºäºå‰ 50 ä¸ªèŠ‚ç‚¹) ---
            state_tensor = paddle.to_tensor(state_for_rl, dtype='float32').unsqueeze(0)
            with paddle.no_grad():
                q_values = agent.main_network(state_tensor).numpy()[0]
            
            valid_q = np.where(mask_for_rl, q_values, -1e9)
            rl_action_idx = np.argmax(valid_q)
            
            # æ˜ å°„å›çœŸå®ç‰©ç† ID (ä» info['node_ids'] é‡Œå–)
            real_node_ids = info.get('node_ids', [])
            if rl_action_idx < len(real_node_ids):
                rl_target_id = real_node_ids[rl_action_idx]
            else:
                rl_target_id = 0
            
            final_target_id = rl_target_id
            decision_type = "RL"

            # --- 2. LLM ä»‹å…¥ (åŸºäºå…¨é‡ 61 ä¸ªèŠ‚ç‚¹) ---
            if ENABLE_LLM and current_desc:
                suggestions = llm.get_suggestions(current_desc) # [55, 12, 52...]
                
                # è¿‡æ»¤æ‰ Mask ä¸º False çš„æ— æ•ˆå»ºè®®
                valid_suggestions = [pid for pid in suggestions if pid < len(raw_mask) and raw_mask[pid]]
                
                if valid_suggestions:
                    top_pick = valid_suggestions[0] # LLM çš„ No.1 æ¨è
                    
                    # [æƒ…å†µ A]: LLM æ¨èäº† RL çœ‹ä¸è§çš„æ–°èŠ‚ç‚¹ (ID >= 50)
                    if top_pick >= TRAIN_NODE_COUNT:
                        final_target_id = top_pick
                        decision_type = "LLM_NEW"
                        stats["LLM_NewNode"] += 1
                        
                    # [æƒ…å†µ B]: LLM æ¨èäº† RL èƒ½çœ‹è§çš„è€èŠ‚ç‚¹ (ID < 50)
                    # åªæœ‰å½“ RL é€‰äº† Cloud (0) æˆ–è€… RL çš„é€‰æ‹©ä¸åœ¨ LLM æ¨èåˆ—è¡¨é‡Œæ—¶ï¼Œæ‰è€ƒè™‘ä¿®æ­£
                    elif rl_target_id == 0: 
                        # æ—¢ç„¶æ˜¯è€èŠ‚ç‚¹ï¼Œè®© RL åœ¨ LLM æ¨èçš„åˆ—è¡¨é‡ŒæŒ‘ä¸€ä¸ª Q å€¼æœ€é«˜çš„
                        # (åˆ©ç”¨ RL çš„å¾®æ“èƒ½åŠ›)
                        visible_suggestions = [pid for pid in valid_suggestions if pid < TRAIN_NODE_COUNT]
                        if visible_suggestions:
                            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ç‰©ç† ID == Action Index
                            best_rescue = max(visible_suggestions, key=lambda x: q_values[x])
                            final_target_id = best_rescue
                            decision_type = "LLM_OPT"
                            stats["LLM_OldNode_Opt"] += 1

            # --- 3. æ‰§è¡ŒåŠ¨ä½œ ---
            # Java ç«¯èƒ½æ¥æ”¶ä»»æ„åˆæ³•çš„ ID (åŒ…æ‹¬ 55)
            next_raw_state, next_raw_mask, reward, done, next_info = env.step(final_target_id)
            
            print(f"\rEp {episode} Step {step} | Act: {final_target_id} ({decision_type}) | R: {reward:.2f}", end="", flush=True)
            
            total_reward += reward
            if done:
                if np.any(raw_mask): total_reward += env.get_final_reward()
                break
            
            # æ›´æ–°çŠ¶æ€
            raw_state = next_raw_state
            raw_mask = next_raw_mask
            info = next_info
            current_desc = info.get('description', "")
            
            # é‡æ–°é€‚é…ç»™ RL
            state_for_rl, mask_for_rl = env.parse_state_adaptive(raw_state, raw_mask)

        print(f"\nEpisode {episode} Done. Reward: {total_reward:.2f} | Stats: {json.dumps(stats)}")

if __name__ == "__main__":
    evaluate()